diff --git a/razor/csrc/aten_cpu_fallback.cpp b/razor/csrc/aten_cpu_fallback.cpp
index 9865644..8474cf0 100644
--- a/razor/csrc/aten_cpu_fallback.cpp
+++ b/razor/csrc/aten_cpu_fallback.cpp
@@ -506,25 +506,25 @@ at::Tensor& AtenRAFTypeDefault::sub_(at::Tensor& self, const at::Scalar& other,
 }
 
 at::Tensor AtenRAFTypeDefault::upsample_nearest2d(
-    const at::Tensor& input, at::OptionalIntArrayRef output_size,
+    const at::Tensor& input, c10::optional<at::IntArrayRef> output_size,
     c10::optional<at::ArrayRef<double>> scale_factors) {
   LTC_FN_TRACK(3);
   LTC_COUNTER("aten::upsample_nearest2d", 1);
   std::vector<at::Tensor> ltc_atens_tensors = {input};
   auto ltc_atens = bridge::LtcCreateTensorList(ltc_atens_tensors);
-  auto&& x_result = at::upsample_nearest2d(ltc_atens[0], output_size.value(), scale_factors);
+  auto&& x_result = at::upsample_nearest2d(ltc_atens[0], output_size, scale_factors);
   return bridge::CreateLtcTensor(x_result, bridge::GetLtcDevice(input));
 }
 
 at::Tensor AtenRAFTypeDefault::upsample_nearest2d_backward(
-    const at::Tensor& grad_output, at::OptionalIntArrayRef output_size,
+    const at::Tensor& grad_output, c10::optional<at::IntArrayRef> output_size,
     at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
   LTC_FN_TRACK(3);
   LTC_COUNTER("aten::upsample_nearest2d_backward", 1);
   std::vector<at::Tensor> ltc_atens_tensors = {grad_output};
   auto ltc_atens = bridge::LtcCreateTensorList(ltc_atens_tensors);
   auto&& x_result =
-      at::upsample_nearest2d_backward(ltc_atens[0], output_size.value(), input_size, scale_factors);
+      at::upsample_nearest2d_backward(ltc_atens[0], output_size, input_size, scale_factors);
   return bridge::CreateLtcTensor(x_result, bridge::GetLtcDevice(grad_output));
 }
 
diff --git a/razor/csrc/aten_cpu_fallback.h b/razor/csrc/aten_cpu_fallback.h
index 6f2ecd1..3c62173 100644
--- a/razor/csrc/aten_cpu_fallback.h
+++ b/razor/csrc/aten_cpu_fallback.h
@@ -7,7 +7,6 @@
 
 #include <ATen/Operators.h>
 #include <ATen/native/CPUFallback.h>
-#include <c10/util/OptionalArrayRef.h>
 
 namespace torch_lazy_tensors {
 
@@ -71,10 +70,10 @@ class AtenRAFTypeDefault {
   static at::Tensor& sub_(at::Tensor& self, const at::Tensor& other, const at::Scalar& alpha);
   static at::Tensor& sub_(at::Tensor& self, const at::Scalar& other, const at::Scalar& alpha);
   static at::Tensor upsample_nearest2d(const at::Tensor& input,
-                                       at::OptionalIntArrayRef output_size,
+                                       c10::optional<at::IntArrayRef> output_size,
                                        c10::optional<at::ArrayRef<double>> scale_factors);
   static at::Tensor upsample_nearest2d_backward(const at::Tensor& grad_output,
-                                                at::OptionalIntArrayRef output_size,
+                                                c10::optional<at::IntArrayRef> output_size,
                                                 at::IntArrayRef input_size,
                                                 c10::optional<at::ArrayRef<double>> scale_factors);
 };
diff --git a/razor/csrc/aten_raf_type.cpp b/razor/csrc/aten_raf_type.cpp
index da34d2f..a46b94f 100644
--- a/razor/csrc/aten_raf_type.cpp
+++ b/razor/csrc/aten_raf_type.cpp
@@ -10,7 +10,6 @@
 #include <ATen/native/Activation.h>
 #include <ATen/native/BinaryOps.h>
 #include <ATen/native/CPUFallback.h>
-#include <c10/util/OptionalArrayRef.h>
 
 #include <mutex>
 
@@ -160,7 +159,7 @@ std::pair<LazyTensor, LazyTensor> GetBinaryOperands(const at::Tensor& self,
 // The input is in format of {N, C, H, W} and the output will be {H, W}.
 std::vector<int64_t> GetOutputSizeWithScale(
     absl::Span<const int64_t> input_size, const c10::optional<at::ArrayRef<double>>& scale_factors,
-    const at::OptionalIntArrayRef& output_size) {
+    const c10::optional<at::IntArrayRef>& output_size) {
   if (!output_size) {
     LTC_CHECK(scale_factors);
     LTC_CHECK_EQ(scale_factors->size(), 2);
@@ -1289,18 +1288,13 @@ at::Tensor LazyNativeFunctions::ge(const at::Tensor& self, const at::Tensor& oth
                                                   bridge::raf_backend::GetLtcTensor(other)));
 }
 
-at::Tensor LazyNativeFunctions::gelu(const at::Tensor& self, c10::string_view approximate) {
+at::Tensor LazyNativeFunctions::gelu(const at::Tensor& self) {
   LTC_FN_COUNTER("raf::");
-  auto gelu_type = at::native::get_gelutype_enum(approximate);
-  LTC_CHECK_EQ(gelu_type, at::native::GeluType::None) << "Not supported GeLU approximation yet";
   return bridge::AtenFromLtcTensor(LazyTensor::gelu(bridge::raf_backend::GetLtcTensor(self)));
 }
 
-at::Tensor LazyNativeFunctions::gelu_backward(const at::Tensor& grad, const at::Tensor& self,
-                                              c10::string_view approximate) {
+at::Tensor LazyNativeFunctions::gelu_backward(const at::Tensor& grad, const at::Tensor& self) {
   LTC_FN_COUNTER("raf::");
-  auto gelu_type = at::native::get_gelutype_enum(approximate);
-  LTC_CHECK_EQ(gelu_type, at::native::GeluType::None) << "Not supported GeLU approximation yet";
   return bridge::AtenFromLtcTensor(LazyTensor::gelu_backward(
       bridge::raf_backend::GetLtcTensor(grad), bridge::raf_backend::GetLtcTensor(self)));
 }
@@ -2520,7 +2514,7 @@ at::Tensor LazyNativeFunctions::std(const at::Tensor& self, at::IntArrayRef dim,
       /*correction=*/unbiased ? 1 : 0));
 }
 
-at::Tensor LazyNativeFunctions::std(const at::Tensor& self, at::OptionalIntArrayRef dim,
+at::Tensor LazyNativeFunctions::std(const at::Tensor& self, c10::optional<at::IntArrayRef> dim,
                                     c10::optional<int64_t> correction, bool keepdim) {
   LTC_FN_COUNTER("raf::");
   LazyTensor self_tensor = bridge::raf_backend::GetLtcTensor(self);
@@ -2762,7 +2756,7 @@ at::Tensor LazyNativeFunctions::upsample_bilinear2d_backward(
 }
 
 at::Tensor LazyNativeFunctions::upsample_nearest2d(
-    const at::Tensor& input, at::OptionalIntArrayRef output_size,
+    const at::Tensor& input, c10::optional<at::IntArrayRef> output_size,
     c10::optional<at::ArrayRef<double>> scale_factors) {
   LTC_FN_COUNTER("raf::");
   LazyTensor input_tensor = bridge::raf_backend::GetLtcTensor(input);
@@ -2775,7 +2769,7 @@ at::Tensor LazyNativeFunctions::upsample_nearest2d(
 }
 
 at::Tensor LazyNativeFunctions::upsample_nearest2d_backward(
-    const at::Tensor& grad_output, at::OptionalIntArrayRef output_size,
+    const at::Tensor& grad_output, c10::optional<at::IntArrayRef> output_size,
     at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
   LTC_FN_COUNTER("raf::");
   LazyTensor grad_output_tensor = bridge::raf_backend::GetLtcTensor(grad_output);
@@ -2839,7 +2833,7 @@ at::Tensor LazyNativeFunctions::var(const at::Tensor& self, at::IntArrayRef dim,
                                                    /*correction=*/unbiased ? 1 : 0, keepdim));
 }
 
-at::Tensor LazyNativeFunctions::var(const at::Tensor& self, at::OptionalIntArrayRef dim,
+at::Tensor LazyNativeFunctions::var(const at::Tensor& self, c10::optional<at::IntArrayRef> dim,
                                     c10::optional<int64_t> correction, bool keepdim) {
   LTC_FN_COUNTER("raf::");
   LazyTensor self_tensor = bridge::raf_backend::GetLtcTensor(self);
diff --git a/razor/csrc/compiler/raf_node_lowering.cpp b/razor/csrc/compiler/raf_node_lowering.cpp
index ca7449e..6d914cf 100644
--- a/razor/csrc/compiler/raf_node_lowering.cpp
+++ b/razor/csrc/compiler/raf_node_lowering.cpp
@@ -906,7 +906,7 @@ TensorValue MakeScalar(T scalar, DType dtype, raf::Device to_dev, std::vector<in
 }
 
 Var RAFNodeLowering::LowerScalar(const ir::ops::Scalar* node) {
-  using at::operator<<;
+  using ir::ops::operator<<;
   using tvm::runtime::DLDataType2String;
   LTC_CHECK_EQ(node->num_outputs(), 1);
   TensorValue tv;
diff --git a/razor/lazy_tensor_core/csrc/ops/scalar.cpp b/razor/lazy_tensor_core/csrc/ops/scalar.cpp
index 3ae1eca..8642848 100644
--- a/razor/lazy_tensor_core/csrc/ops/scalar.cpp
+++ b/razor/lazy_tensor_core/csrc/ops/scalar.cpp
@@ -44,6 +44,10 @@ lazy_tensors::hash_t ScalarHash(const at::Scalar& s) {
                              : lazy_tensors::util::Hash(s.toLong());
 }
 
+std::ostream& operator<<(std::ostream& ostrm, at::Scalar s) {
+  return ostrm << (s.isFloatingPoint() ? s.toDouble() : s.toLong());
+}
+
 }  // namespace ops
 }  // namespace ir
 }  // namespace torch_lazy_tensors
diff --git a/razor/lazy_tensor_core/csrc/ops/scalar.h b/razor/lazy_tensor_core/csrc/ops/scalar.h
index ce5c296..9b9c773 100644
--- a/razor/lazy_tensor_core/csrc/ops/scalar.h
+++ b/razor/lazy_tensor_core/csrc/ops/scalar.h
@@ -7,7 +7,6 @@
 
 #pragma once
 
-#include <ATen/core/Formatting.h>
 #include <c10/core/Scalar.h>
 
 #include <iostream>
@@ -42,6 +41,8 @@ class Scalar : public Node {
 
 lazy_tensors::hash_t ScalarHash(const at::Scalar& s);
 
+std::ostream& operator<<(std::ostream& ostrm, at::Scalar s);
+
 }  // namespace ops
 }  // namespace ir
 }  // namespace torch_lazy_tensors
